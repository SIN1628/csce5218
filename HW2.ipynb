{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
        "\n",
        "# **HW1a The Perceptron** (20 pt)\n"
      ],
      "metadata": {
        "id": "vYiZq0X2oB5t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "# Get the datasets\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/train.dat\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-20 17:54:31--  http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2844 (2.8K)\n",
            "Saving to: ‘test.dat.2’\n",
            "\n",
            "\rtest.dat.2            0%[                    ]       0  --.-KB/s               \rtest.dat.2          100%[===================>]   2.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-20 17:54:32 (264 MB/s) - ‘test.dat.2’ saved [2844/2844]\n",
            "\n",
            "--2023-02-20 17:54:32--  http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11244 (11K)\n",
            "Saving to: ‘train.dat.2’\n",
            "\n",
            "train.dat.2         100%[===================>]  10.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-20 17:54:32 (239 MB/s) - ‘train.dat.2’ saved [11244/11244]\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGVmKzgG2Ium",
        "outputId": "66e6cfdc-dd2f-48ac-975f-84d9e96136aa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "# Take a peek at the datasets\n",
        "!head train.dat\n",
        "!head test.dat"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
            "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t0\n",
            "0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A69DxPSc8vNs",
        "outputId": "f2a6ae46-104f-46c6-d513-308b1df88242"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the Perceptron Model\n",
        "\n",
        "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
      ],
      "metadata": {
        "id": "rFXHLhnhwiBR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "import math\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "\n",
        "# Corpus reader, all columns but the last one are coordinates;\n",
        "#   the last column is the label\n",
        "def read_data(file_name):\n",
        "    f = open(file_name, 'r')\n",
        "\n",
        "    data = []\n",
        "    # Discard header line\n",
        "    f.readline()\n",
        "    for instance in f.readlines():\n",
        "        if not re.search('\\t', instance): continue\n",
        "        instance = list(map(int, instance.strip().split('\\t')))\n",
        "        # Add a dummy input so that w0 becomes the bias\n",
        "        instance = [-1] + instance\n",
        "        data += [instance]\n",
        "    return data\n",
        "\n",
        "\n",
        "def dot_product(array1, array2):\n",
        "    return sum(x * y for x, y in zip(array1, array2))\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 * (1 + math.exp( -x))\n",
        "\n",
        "# The output of the model, which for the perceptron is \n",
        "# the sigmoid function applied to the dot product of \n",
        "# the instance and the weights\n",
        "def output(weight, instance):\n",
        "    linear_combination = dot_product(weight, instance[:-1])\n",
        "    output = sigmoid(linear_combination)\n",
        "    return output\n",
        "\n",
        "# Predict the label of an instance; this is the definition of the perceptron\n",
        "# you should output 1 if the output is >= 0.5 else output 0\n",
        "def predict(weights, instance):\n",
        "    prediction = sigmoid(dot_product(weights, instance))\n",
        "    if prediction >= 0.5:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "# Accuracy = percent of correct predictions\n",
        "def get_accuracy(weights, instances):\n",
        "    # You do not to write code like this, but get used to it\n",
        "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
        "                   for instance in instances])\n",
        "    return correct * 100 / len(instances)\n",
        "\n",
        "\n",
        "# Train a perceptron with instances and hyperparameters:\n",
        "#       lr (learning rate) \n",
        "#       epochs\n",
        "# The implementation comes from the definition of the perceptron\n",
        "#\n",
        "# Training consists on fitting the parameters which are the weights\n",
        "# that's the only thing training is responsible to fit\n",
        "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
        "#\n",
        "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
        "# We are updating weights in the opposite direction of the gradient of the error,\n",
        "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
        "def train_perceptron(instances, lr, epochs):\n",
        "    weights = [0] * (len(instances[0])-1)  # Initialize weights to all zeros\n",
        "    for _ in range(epochs):\n",
        "        for instance in instances:\n",
        "            in_value = dot_product(weights, instance[:-1])  # Calculate input value\n",
        "            output = sigmoid(in_value)  # Apply sigmoid activation function\n",
        "            error = instance[-1] - output  # Calculate prediction error\n",
        "            # Update weights according to Perceptron learning rule\n",
        "            for i in range(0, len(weights)):\n",
        "                weights[i] += lr * error * output * (1 - output) * instance[i]\n",
        "    return weights\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "cXAsP_lw3QwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run it"
      ],
      "metadata": {
        "id": "adBZuMlAwiBT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "lr = 0.005\n",
        "epochs = 5\n",
        "weights = train_perceptron(instances_tr, lr, epochs)\n",
        "accuracy = get_accuracy(weights, instances_te)\n",
        "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n"
          ]
        }
      ],
      "metadata": {
        "id": "50YvUza-BYQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ad14f8-0ffc-4211-f6a7-fd549ab75594"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "Answer the following questions. Include your implementation and the output for each question."
      ],
      "metadata": {
        "id": "CBXkvaiQMohX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Question 1\n",
        "\n",
        "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
        "```\n",
        "in_value = dot_product(weights, instance)\n",
        "output = sigmoid(in_value)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "Why don't we have the following code snippet instead?\n",
        "```\n",
        "output = predict(weights, instance)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "#### TODO Add your answer here (text only)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YCQ6BEk1CBlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1A)The only variation between the two code fragments is a small one. In the first code snippet, the output is produced using the sigmoid function after the value is calculated as the dot product of the weights and the instance. The error is determined as the difference between the real label (instance [-1]) and the predicted label in the second code snippet, where the output is produced using the predict function (output). These lines of code are meant to figure out the discrepancy between the real label and the anticipated label so that the weights may be adjusted to lessen the mistake. The difference between the actual label and the anticipated label is used to determine the error in both scenarios, which is the same thing. Therefore, the only difference between the two code snippets is whether the prediction is done using the predict function or the sigmoid function. The specific implementation and model's design would determine which one should be used. The application of the sigmoid function is followed by the prediction being generated using the dot product of the weights and the instance in the first code snippet, which is more specifically designed for a perceptron model."
      ],
      "metadata": {
        "id": "ku-X-2CozkNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2\n",
        "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
        "\n",
        "```\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
        "lr = [0.005, 0.01, 0.05]              # learning rate\n",
        "```\n",
        "\n",
        "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
        "of your code.The output should look like the following:\n",
        "```\n",
        "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "[and so on for all the combinations]\n",
        "```\n",
        "You will get different results with different hyperparameters.\n",
        "\n",
        "#### TODO Add your answer here (code and output in the format above) \n"
      ],
      "metadata": {
        "id": "JU3c3m6YL2rK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
        "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
        "\n",
        "for lr in lr_array:\n",
        "  for tr_size in tr_percent:\n",
        "    for epochs in num_epochs:\n",
        "      size =  round(len(instances_tr)*tr_size/100)\n",
        "      pre_instances = instances_tr[0:size]\n",
        "      weights = train_perceptron(pre_instances, lr, epochs)\n",
        "      accuracy = get_accuracy(weights, instances_te)\n",
        "    print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
            "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 32.0\n",
            "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n"
          ]
        }
      ],
      "metadata": {
        "id": "G-VKJOUu2BTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "764b1750-48a6-421f-9efe-e9fc86678772"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3\n",
        "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
        "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
        "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
        "   ```\n",
        "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
        "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "```\n",
        "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
        "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
        "\n",
        "#### TODO: Add your answer here (code and text)\n",
        "\n"
      ],
      "metadata": {
        "id": "OFB9MtwML24O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "38rA_Kp3wiBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3A) The accuracy of a machine learning model is affected by several factors, including the size of the training dataset, the complexity of the model, the quality of features, and the presence of overfitting or excessive. -adjustment. However, in general, a larger training dataset can lead to higher accuracy on the test dataset, assuming the model is well designed and can generalize well to new data, yet see.\n",
        "However, increasing the size of the training dataset does not always lead to improved accuracy. In some cases, this can even lead to redundant pages, where the model becomes too complex and performs well on the training data but poorly on the test data. In such cases, other techniques such as regularization or early stopping can be used to solve the redundancy problem. In summary, the relationship between the size of the training dataset and the accuracy of the test dataset is complex and depends on a number of factors. Using only a large training dataset does not guarantee high accuracy, and a well-designed model that can generalize well to the new data is important for achieving high accuracy."
      ],
      "metadata": {
        "id": "BAjwLrHJzuXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3B) There can be several reasons why a model trained on a larger training dataset (200 instances) can have worse accuracy (68.0%) compared to a model trained on a smaller training dataset (100 instances) with an accuracy of 71.0%. Some of the reasons can be: OVERFITTING: More data increases the likelihood that the model will memorize the training set and underperform on the test set. Overfitting is what is happening here, and it can make tests less accurate.\n",
        "LEARNING RATE: In comparison to the first run, the second run shows a lower learning rate (0.005). (0.050). The model may converge too slowly and be less efficient at recognizing patterns in the data if the learning rate is lower. A higher learning rate, on the other hand, may lead the model to fluctuate and fail to reach the best outcome. It can be difficult to strike the perfect balance between these two extremes.\n",
        "NUMBER OF EPOCHS: The model is trained for 20 epochs in each run. To make sure that the model has seen every example in the training data, a higher number of epochs can be required for a bigger training dataset. On the other side, overfitting can also result from a large increase in the number of epochs. In conclusion, a machine learning model's accuracy is impacted by a variety of parameters, and merely enlarging the training dataset won't result in an increase in accuracy. For reaching high accuracy, other factors including a well-designed model, a good learning rate, and an acceptable number of epochs are crucial."
      ],
      "metadata": {
        "id": "ea3U56VUz4-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3C) Yes, adding more hyperparameters can lead to improved accuracy, but this is not a given. The quantity and quality of the training dataset, the architecture of the model, the features used, and the values of the hyperparameters are some of the variables that affect how accurate a machine learning model is.\n",
        "In some situations, adding more hyperparameters might increase the model's accuracy by enabling it to learn more intricate data representations. This raises the possibility of overfitting, a situation in which a model is too sophisticated and performs well on training data but badly on test data.\n",
        "A mix of methods, such as feature engineering, model selection, hyperparameter tweaking, and regularization, can be utilized to increase accuracy. For instance, increasing accuracy may be achieved by optimizing the hyperparameters using methods like cross-validation, grid search, or random search.\n",
        "In conclusion, accuracy higher than 80.0% can be attained by tuning the hyperparameters, but it also depends on the quality of the data, the model, and the features chosen."
      ],
      "metadata": {
        "id": "99bhmiszz9k7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3D) No, it isn't always beneficial to train for additional epochs. The size and quality of the training dataset, the model architecture used, the features selected, and the values of the hyperparameters, including the number of epochs, all affect how accurate a machine learning model is.\n",
        "Underfitting, when the model has not seen enough samples to learn the patterns in the data, can occur from training for too few epochs. On the other side, overfitting, which occurs when the model grows too complicated and memorizes the training data instead of generalizing to new, unknown data, may be caused by training for too many epochs.\n",
        "The size of the training dataset, the model's complexity, the caliber of the features, and the existence of overfitting or underfitting are some of the variables that affect the ideal number of epochs. In general, increasing the number of epochs can reduce training loss, but after a given number of epochs, test accuracy may begin to plateau.\n",
        "The ideal number of epochs varies on a variety of parameters and preparing for additional epochs is not always worthwhile. The ideal number of epochs for a particular dataset and model may be determined by keeping an eye on the training loss and test accuracy, utilizing methods like early stopping, and monitoring the results."
      ],
      "metadata": {
        "id": "7zvz5_bV0DHA"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
